{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy (Uncertainty)\n",
    "\n",
    "You are a code breaker during World War 2. Every day you intercept a series of bits one at a time, for simplicities sake lets assume there are only 4 bits, $\\{A, B, C, D\\}$. You have been a code breaker for a long time so you know the probabilities of each bit to be\n",
    "\n",
    "$$P(x_i = A) = \\frac{1}{2} \\quad P(x_i = B) = \\frac{1}{4} \\quad P(x_i = C) = \\frac{1}{8} \\quad P(x_i = D) = \\frac{1}{8}$$\n",
    "\n",
    "What is the next bit? How certain are you? Your certainty describes the entropy or certainty of the system. If you know  what the next bit is going to be, \n",
    "\n",
    "$$P(x_i = A) = 1 \\quad P(x_i = B) = 0 \\quad P(x_i = C) = 0 \\quad P(x_i = D) = 0$$\n",
    "\n",
    "then the entropy is 0. Logically then, entropy is greatest when you are the most uncertain. For example if the probabilities are uniformly distributed, given that you are no more confident that the answer is $A$, than you are $B$, $C$, or $D$.\n",
    "\n",
    "$$P(x_i = A) = \\frac{1}{4} \\quad P(x_i = B) = \\frac{1}{4} \\quad P(x_i = C) = \\frac{1}{4} \\quad P(x_i = D) = \\frac{1}{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy (Guessing)\n",
    "Returning to the above code breaker example, we originally asked what you, \"What is the next bit\"? Lets take the original pmf \n",
    "\n",
    "$$P(x_i = A) = \\frac{1}{2} \\quad P(x_i = B) = \\frac{1}{4} \\quad P(x_i = C) = \\frac{1}{8} \\quad P(x_i = D) = \\frac{1}{8}$$\n",
    "\n",
    "You would first guess $A$, then $B$, then $C$ or $D$ and so on until you were correct. Thus on average it would take you\n",
    "\n",
    "$$\\frac{1}{2} \\cdot 1 + \\frac{1}{4} \\cdot 2 + \\frac{1}{8} \\cdot 3 + \\frac{1}{8} \\cdot 3 = 1 \\frac{3}{4}$$\n",
    "\n",
    "$1.75$ guesses, in this context entropy is the same as guessing, thus the entropy of the above sequence would be $1.75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy (Formula)\n",
    "\n",
    "What then is the formula for the entropy of a system? (note that we denote Entropy as $H$ (capital eta))\n",
    "$$ H = -\\sum_i^n P(x_i) \\cdot \\log_2(P(x_i)) $$\n",
    "\n",
    "By itself this doesn't really make any sense, so lets rewrite it\n",
    "\n",
    "$$-\\sum_i^n P(x_i) \\cdot \\log_2(P(x_i)) = \\sum_i^n P(x_i) \\cdot -\\log_2(P(x_i)) = \\sum_i^n P(x_i) \\cdot \\log_2 \\left(\\frac{1}{P(x_i)}\\right) = E\\left[\\log_2 \\left(\\frac{1}{P(x_i)}\\right)\\right]$$\n",
    "\n",
    "The function which we are computing the expecation of $\\log_2 \\left(\\frac{1}{P(x_i)}\\right)$ is called *information* or *surprisal*, and its really the root of information theory.\n",
    "\n",
    "> *The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprisal Intuition\n",
    "Surprisal is defined as\n",
    "\n",
    "$$\\log_2 \\left(\\frac{1}{P(x_i)}\\right)$$\n",
    "\n",
    "There is some basic intuition for each term\n",
    "\n",
    "- $log_2(N)$ is the number of bits needed to represent an integer $N$\n",
    "- $\\frac{1}{P(x_i = A)}$ is the geometric expectation. In other words $\\frac{1}{P(x_i = A)}$ is the number of bits we can expect to see before we see the $A$ bit\n",
    "\n",
    "All together we can also interpret surprisal as we did above, as the number of \"optimal guesses\" required"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

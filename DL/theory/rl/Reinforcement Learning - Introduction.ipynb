{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "- $\\theta$ denotes the parameters of our function\n",
    "- $\\sim$ denotes \"distributed as\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States vs Observations\n",
    "A **State** provides a complete description of the enviroment. An **Observation** provides a partial description of the enviroment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete vs Continuous (Action Spaces)\n",
    "A **Discrete** action space has a finite number of moves. A **Continuous** action space has an infinite number of moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic vs Stochastic (Policies)\n",
    "A policy is a function, which takes a state $s_t$ and returns an action, $a_t$\n",
    "\n",
    "If the policy is **deterministic**, the function is denoted $\\mu$\n",
    "$$a_t = \\mu_\\theta(s_t)$$\n",
    "\n",
    "If the policy is **stochastic**, the function is denoted $\\pi$\n",
    "$$a_t \\sim \\pi_\\theta(s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectories\n",
    "Trajectories are a sequence of states and actions\n",
    "$$\\tau = (s_0, a_0, s_1, a_1, \\ldots, s_n, a_n)$$\n",
    "\n",
    "State transitions (whatever happens between $s_t$ and $s_{t+1}$) are governed by the natural laws of the enviroment, and depend on the most recent action $a_t$.\n",
    "\n",
    "If $a_t$ is **determininistic** then the subsequent $s_{t+1}$ is deterministic\n",
    "$$s_{t+1} = f(s_t, a_t)$$\n",
    "\n",
    "If $s_t$ is **stochastic** then the subsequent $s_{t+1}$ is stochastic\n",
    "$$s_{t+1} \\sim P(\\cdot | s_t, a_t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward and Return\n",
    "\n",
    "The reward function, $r_t$, depends on the current state $s_t$, the action taken, $a_t$, and the subsequent state $s_{t+1}$\n",
    "\n",
    "$$r_t = R(s_t, a_t, s_{t+1})$$\n",
    "\n",
    "The goal of an agent is to maximize reward over a trajectory, $R(\\tau)$. \n",
    "\n",
    "Maximizing the reward over a fixed range is called, **finite-horizon undiscounted return**\n",
    "$$R = \\sum_{t=0}^T r_t$$\n",
    "\n",
    "Maximizing the reward over an infinite range is called, **infinite-horizon discounted return** \n",
    "$$R = \\sum_{t=0}^\\infty \\gamma^t \\cdot r_t$$\n",
    "($\\gamma \\in (0, 1)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RL Problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "- $\\theta$ denotes the parameters of our function\n",
    "- $\\sim$ denotes \"distributed as\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States vs Observations\n",
    "A **State** provides a complete description of the enviroment. An **Observation** provides a partial description of the enviroment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete vs Continuous (Action Spaces)\n",
    "A **Discrete** action space has a finite number of moves. A **Continuous** action space has an infinite number of moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic vs Stochastic (Policies)\n",
    "A policy is a function, which takes a state $s_t$ and returns an action, $a_t$\n",
    "\n",
    "If the policy is **deterministic**, the function is denoted $\\mu$\n",
    "$$a_t = \\mu_\\theta(s_t)$$\n",
    "\n",
    "If the policy is **stochastic**, the function is denoted $\\pi$\n",
    "$$a_t \\sim \\pi_\\theta(s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectories\n",
    "Trajectories are a sequence of states and actions\n",
    "$$\\tau = (s_0, a_0, s_1, a_1, \\ldots, s_n, a_n)$$\n",
    "\n",
    "State transitions (whatever happens between $s_t$ and $s_{t+1}$) are governed by the natural laws of the enviroment, and depend on the most recent action $a_t$.\n",
    "\n",
    "If $a_t$ is **determininistic** then the subsequent $s_{t+1}$ is deterministic\n",
    "$$s_{t+1} = f(s_t, a_t)$$\n",
    "\n",
    "If $s_t$ is **stochastic** then the subsequent $s_{t+1}$ is stochastic\n",
    "$$s_{t+1} \\sim P_{s, a}$$\n",
    "Where $P_{s, a}$ gives the distribution over what states we will transition to if we take action $a$ in state $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward and Return\n",
    "\n",
    "The reward function, $r_t$, depends on the current state $s_t$, the action taken, $a_t$, and the subsequent state $s_{t+1}$\n",
    "\n",
    "$$r_t = R(s_t, a_t, s_{t+1})$$\n",
    "\n",
    "The goal of an agent is to maximize reward over a trajectory, $R(\\tau)$. \n",
    "\n",
    "Maximizing the reward over a fixed range is called, **finite-horizon undiscounted return**\n",
    "$$R = \\sum_{t=0}^T r_t$$\n",
    "\n",
    "Maximizing the reward over an infinite range is called, **infinite-horizon discounted return** \n",
    "$$R = \\sum_{t=0}^\\infty \\gamma^t \\cdot r_t$$\n",
    "($\\gamma \\in (0, 1)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Value Function\n",
    "\n",
    "The dynamics of our R.L process are as follows: We start in some state $s_0$,\n",
    "and get to choose some action $a_0 \\in A$. As a result of our\n",
    "choice, the state randomly transitions to some successor state\n",
    "$s_1$, drawn according to $s1 \\sim P_{s_0,a_0}$. Then, we get to pick another action $a_1$.\n",
    "As a result of this action, the state transitions again, now to some $s_2 \\sim P_{s_1,a_1}$.\n",
    "We then pick $a_2$, and so onâ€¦. Pictorially, we can represent this process as\n",
    "follows:\n",
    "$$s_0 \\stackrel{a_0}{\\to} s_1 \\stackrel{a_1}{\\to} \\ldots$$\n",
    "\n",
    "Our reward function being\n",
    "$$R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\ldots$$\n",
    "\n",
    "Next we introduce $V^{\\pi_\\theta}$ which denotes the **value function** tied to a given policy\n",
    "$$E\\left[R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\ldots ~|~ s_0=s, \\pi\\right] = V^{\\pi_\\theta}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Optimal Value Function\n",
    "The thing we are trying to find, or the Optimal Value Function is written\n",
    "\n",
    "$$V^*(s) = \\max V^\\pi (s)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between a vanilla RNN, and an LSTM\n",
    "LSTMs are a kind of RNN. How are they different from a Vanilla RNN?\n",
    "\n",
    "- In a Vanilla RNN, we have a hidden state which we pass forward\n",
    "$$\n",
    "h_t = \\tanh\\left( W \\cdot \\begin{bmatrix} h_{t-1} \\\\ x \\end{bmatrix}\\right)\n",
    "$$\n",
    "\n",
    "- In an LSTM, we have a sequence of gates $i$, $f$, $o$, $g$, which each control the increment to the subsequent hidden state\n",
    "$$\n",
    "\\begin{bmatrix} i \\\\ f \\\\ o \\\\ g\\end{bmatrix} = \\begin{bmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\tanh\\end{bmatrix} \\cdot W \\cdot \\begin{bmatrix} h_{t-1} \\\\ x\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Formulation\n",
    "Functionally we use each of these \"gates\" to compute the following updates\n",
    "$$c_t = f \\odot c_{t-1} + i \\odot g$$\n",
    "$$h_t = o \\odot \\tanh(c_t)$$\n",
    "note that $c_t$ is short for \"cell state\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the Gates\n",
    "\n",
    "| character \t| full name   \t| range of values \t| role                                                                               \t|\n",
    "|-----------\t|-------------\t|-----------------\t|------------------------------------------------------------------------------------\t|\n",
    "| i         \t| input gate  \t| 0 to 1          \t| Which cell states to we want to increment \t|\n",
    "| f         \t| forget gate \t| 0 to 1          \t| What to erase from the previous cell state                                         \t|\n",
    "| o         \t| output gate \t| 0 to 1          \t| What to pass forward from the cell state to the hidden state                       \t|\n",
    "| g         \t| gate        \t| -1 to 1         \t| By how much do we want to increment each cell state                                \t|\n",
    "\n",
    "Remember that each of the gates corresponds to the increment of a single elements\n",
    "\n",
    "$$c_t[0][0] = f[0][0] \\cdot c_{t-1}[0][0] + i[0][0] \\cdot g[0][0]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
    "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
    "         \n",
    "    def forward(self, x, h_t, c_t):\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "         \n",
    "        HS = self.hidden_size\n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            gates = x_t @ self.W + h_t @ self.U + self.bias\n",
    "            i_t, f_t, g_t, o_t = (\n",
    "                torch.sigmoid(gates[:, :HS]), # input\n",
    "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
    "                torch.tanh(gates[:, HS*2:HS*3]), # gate\n",
    "                torch.sigmoid(gates[:, HS*3:]), # output\n",
    "            )\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "Torch provides a nn.LSTM module its usage is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features_inp (embedding size) (100)\n",
    "# hidden_features (256)\n",
    "# num_layers (2)\n",
    "lstm = nn.LSTM(100, 256, 2)\n",
    "\n",
    "# shape = seq_len, batch_size, num_features\n",
    "x = torch.ones((100, 1, 100))\n",
    "\n",
    "output, (hidden, cell) = lstm(x)\n",
    "output.shape, hidden.shape, cell.shape\n",
    "\n",
    "# output[i] = hidden state i of the last layer\n",
    "# hidden[i] = the last hidden state of the ith layer\n",
    "# cell[i] = the last cell state of the ith layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[Stanford RNN Lecture](https://www.youtube.com/watch?v=6niqTuYFZLQ&t=3445s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

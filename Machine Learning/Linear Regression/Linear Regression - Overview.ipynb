{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup / Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(low=1, high=10, size=(20, 3))\n",
    "y = 1*X[:, 0] + 2*X[:, 1] + 1*X[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The goal of a linear regression is to fit a hypothesis, $h(t)$ as a linear combination of terms\n",
    "$$\\theta_0 + \\theta_1 \\cdot x_1 + \\ldots + \\theta_n \\cdot x_n = h(x)$$\n",
    "\n",
    "The loss function , $j$ which we choose will effect what kind of linear regression which we are doing. For example\n",
    "- $J(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (h(x^{(i)}) - y)^2$, OLS or \"Ordinary Least Squares\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Derivation\n",
    "Lets work out the Gradient for our \"Ordinary Least Squares\" function\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J_\\theta = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} \\sum_{i=1}^n (h(x^{(i)}) - y)^2$$\n",
    "\n",
    "To make the derivation simpler lets change $ \\frac{1}{2} \\sum_{i=1}^n (h(x^{(i)}) - y)^2$ to $ \\frac{1}{2} (h(x^{(i)}) - y)^2$ thus computing the gradient over a single training example $i$ rather than all $n$ training examples\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J_\\theta = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} (h(x^{(i)}) - y)^2$$\n",
    "\n",
    "By the chain rule, this then breaks into\n",
    "$$= \\frac{1}{2} \\cdot 2 \\cdot (h(x^{(i)}) - y) \\cdot \\frac{\\partial}{\\partial \\theta_j} (h(x^{(i)}) - y)$$\n",
    "$$= (h(x^{(i)}) - y) \\cdot (x_j) $$\n",
    "\n",
    "Note: Remember that the partial derivative of any variable other than the variable with whom you are differentiating is equivalent to differentiating a constant and yields $0$. Hence why in this instance, when we differentiate by the constant $\\theta_j$, the only term left is its multiple $x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Rule\n",
    "Gradient Descent, by definition, is\n",
    "$$\\theta_j := \\theta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j} J_\\theta$$\n",
    "(Where $\\alpha$ is a parameter called learning rate, which controls the step size of each gradient update)\n",
    "\n",
    "Plugging in our above equation we get our first OLS Linear Regression model\n",
    "$$\\theta_j := \\theta_j - \\alpha \\cdot (h(x^{(i)}) - y) \\cdot (x_j)$$ \n",
    "or\n",
    "$$\\theta_j := \\theta_j + \\alpha \\cdot (y- h(x^{(i)})) \\cdot (x_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try it out our derivation\n",
    "\n",
    "def loss(h, x, y, alpha):\n",
    "    # x is a vector (training sample)\n",
    "    # y is a scalar\n",
    "    # alpha is a scalar\n",
    "    \n",
    "    return (alpha * (y - h)) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_gradient_descent(X, y):\n",
    "    epochs, alpha = 1000, .0001\n",
    "    (n_rows, n_features) = X.shape\n",
    "    theta = np.zeros(n_features)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for x, y_i in zip(X, y):\n",
    "            # estimate value\n",
    "            h = np.dot(theta, x)\n",
    "            \n",
    "            # compute loss\n",
    "            loss_value = loss(h, x, y_i, alpha)\n",
    "            \n",
    "            # update weights\n",
    "            theta = theta + loss_value\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000011, 1.99999802, 1.00000201])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = simple_gradient_descent(X, y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "Our original algoritm is slow because we are computing the gradient at each step. What if instead we took an average of the squared residual of all of our training samples? Or in math terms\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J_\\theta = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2n} \\sum_{i=1}^n (h(x^{(i)}) - y)^2$$\n",
    "\n",
    "We already know (from above)\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} (h(x^{(i)}) - y)^2 = (h(x^{(i)}) - y) \\cdot (x_j)$$\n",
    "\n",
    "Therefore \n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2n} \\sum_{i=1}^n (h(x^{(i)}) - y)^2 = \\frac{1}{n} \\sum_{i=1}^n (h(x^{(i)}) - y) \\cdot (x_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y):\n",
    "    (n_rows, n_features) = X.shape\n",
    "    theta = np.zeros(n_features)\n",
    "    alpha, epochs = .0001, 1000\n",
    "    N = len(y)  # number of training examples\n",
    "    for i in range(epochs):\n",
    "        y_hat = np.dot(X, theta)\n",
    "        theta = theta - alpha * (1.0/N) * np.dot(X.T, y_hat-y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.11764269, 1.70510187, 1.20338965])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gradient_descent(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Normal Equations\n",
    "\n",
    "Its worth noting that in this very specific circumstance there exists a closed form way to compute $\\theta$ without doing so iteratively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "$x^{(i)}$ denotes the *ith* row in the matrix $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Map Definition\n",
    "\n",
    "A feature map, $\\phi$ can be used to map our $d$ dimensional input vector $x^{(i)}$ into a higher dimensional space. For example\n",
    "\n",
    "$$\n",
    "\\phi(x) = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_2 \\cdot x_1\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\phi\\left( \\begin{bmatrix} 2 & 4 \\end{bmatrix}\\right) = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\\\\\n",
    "4\\\\\n",
    "4 \\cdot 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And thus \n",
    "$$h_\\theta(x^{(i)}) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_2 \\cdot x_1 \\implies h_\\theta(x^{(i)}) = \\theta^T \\cdot \\phi(x^{(i)})$$\n",
    "\n",
    "The update rule for batch SGD using a map $\\phi$ is\n",
    "\n",
    "$$\\theta := \\theta + \\alpha \\cdot \\sum_{i}^N (y - \\theta^T \\cdot \\phi(x^{(i)})) \\cdot \\phi(x^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Runtime Complexity\n",
    "The update rule for batch SGD is \n",
    "$$\\theta := \\theta + \\alpha \\cdot \\sum_{i}^N (y - \\theta^T \\cdot \\phi(x^{(i)})) \\cdot \\phi(x^{(i)})$$\n",
    "\n",
    "The runtime complexity is $N \\cdot P$ where $P$ is the number of parameters ($\\| \\phi \\| = P$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Number of Parameters\n",
    "In traditional Linear Regression, \n",
    "$$h_\\theta(x^{(i)}) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = \\theta^T \\cdot x^{(i)}$$\n",
    "\n",
    "there is one parameter for every dimension, $\\| \\theta \\| = \\| \\phi \\| = \\| x^{(i)} \\|$, so optimizing using SGD is manageable. This some times simply is not enough. \n",
    "\n",
    "Imagine we are trying to predict housing prices and we know the square footage and price. A common indicator is real estate is price per square foot or ppf. We could create a new column $x_3 = \\frac{x_2}{x_1}$ and the re-fit linear regression, but imagine we had dozens or even hundreds of variables pertaining to the value of a house, and we wanted to know each of their ratios, \n",
    "\n",
    "$$\\phi(X) = \\begin{bmatrix}\n",
    "1\\\\\n",
    "x_1\\\\\n",
    "\\ldots\\\\\n",
    "x_n \\\\\n",
    "\\frac{x_1}{x_2}\\\\\n",
    "\\ldots\\\\\n",
    "\\frac{x_{n}}{x_{n-1}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "this system would quickly become inefficient as the number of parameters would be growing exponentially w.r.t (with respect to), the number of dimensions $\\| \\theta \\| = \\| \\phi \\|= \\approx (\\| x^{(i)} \\|)^2$, and as a result SGD runtime complexity would also expload. A dataset with $1000$ columns would suddenly have $1,000,000$ different features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel Trick\n",
    "Fitting functions which large maps is expensive $O(NP)$, one way to reduce the complexity would be to find a way to express that same function, but in fewer parameters. The Kernel Trick solves for this by rewriting $\\theta$ as a sum of all records multiplied by some constant, $\\beta_i$\n",
    "\n",
    "$$\\theta = \\sum_{i}^N \\beta_i \\cdot \\phi(x^{(i)}) \\quad \\beta_i \\in \\mathbb{R} $$\n",
    "\n",
    "Note that if we proved this was true, then we would be rewriting $\\theta$ in terms of $N$ parameters $\\{\\beta_1, \\beta_2, \\ldots, \\beta_N\\}$, rather than the original $P$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel Trick Proof\n",
    "W.T.S\n",
    "$$\\theta = \\sum_{i}^N \\beta_i \\cdot \\phi(x^{(i)}) \\quad \\beta_i \\in \\mathbb{R} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new algorithm\n",
    "\n",
    "During the **N+1** step, we subtituted $\\alpha \\cdot (y^{(i)} - \\theta^T \\cdot \\phi(X))$ for $\\beta_{N + 1}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

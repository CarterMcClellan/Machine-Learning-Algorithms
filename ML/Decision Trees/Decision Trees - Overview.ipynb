{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 2 columns = features\n",
    "# third column = label\n",
    "X = np.zeros((30, 3))\n",
    "\n",
    "# class 0 = normally distributed mu=2, sigma=.1\n",
    "X[:15, :2] = np.random.normal(loc=2, scale=.5, size=(15, 2))\n",
    "\n",
    "# class 1 = normally distributed mu=4, sigma=.1\n",
    "X[15:, :2] = np.random.normal(loc=4, scale=1, size=(15, 2))\n",
    "X[15:, 2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "You want to build a classifier. But unlike logistic regression, which uses a continuous function to model discrete results, you want to use a discrete set of rules to classify your input variable. A trivial example might be to classify whether or not you want to play tennis\n",
    "\n",
    "- if the weather is sunny\n",
    "    - if the temperature is below 70 -> don't play tennis\n",
    "    - if the temperature is above 70 -> play tennis\n",
    "- if the weather is cloudy\n",
    "    - if the temperature is below 70 -> don't play tennis\n",
    "    - if the temperature is above 70 -> don't play tennis\n",
    "    \n",
    "if we encoded our the problem (1 = sunny, 2 = cloudy), (2nd variable representing temperature), (1 = play tennis, 0 = dont play tennis)\n",
    "\n",
    "```\n",
    "(1, 68, 0)\n",
    "(1, 82, 1)\n",
    "(2, 66, 0)\n",
    "(2, 90, 0)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy comes into play\n",
    "\n",
    "Entropy is the expectation, or average surprisal. The term surprisal comes from how surprised you might be by an event, in an urn with 99 red balls, and 1 green ball, pulling a green ball would be a big surprise. Note that a urn with 99 red balls and 1 green ball would be a low entropy urn, given that 99 times out of 100, you would be correct and entropy refers to the average.\n",
    "\n",
    "Now lets apply this to classification. You want to divide your days into 2 groups, good for tennis and bad for tennis. We want all the good days to be on one side of the classification boundry, and all of the bad days to be on the other, thus we want to sort all of our days into 2 groups in such a way that entropy is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy (Loss Function)\n",
    "$$ H = E\\left[\\log_2 \\left(\\frac{1}{P(x_i)}\\right)\\right]= -\\sum_i^n P(x_i) \\cdot \\log_2(P(x_i)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing Entropy\n",
    "Decisions trees dont have a fancy way of minimizing their loss function. Its just an exhaustive search.\n",
    "For each feature, $f_i$, explore all possible split values, then calculate the entropy of the split. After exploring all possible splits, choose the best option. Lets see this in action with our example\n",
    "\n",
    "Weather Splits.\n",
    "- Split on Sunny\n",
    "    - Group 1: (1, 68, 0), (1, 82, 1)\n",
    "    - Group 2: (2, 66, 0), (2, 90, 0)\n",
    "\n",
    "Entropy of Group 2 $= 0$, Entropy of Group 1 $= 1$. Note that these groups will be exactly the same for Split on Cloudy. Next we explore the temperature splits\n",
    "\n",
    "- Split on 68\n",
    "    - Group 1: (1, 68, 0), (2, 66, 0)\n",
    "    - Group 2: (1, 82, 1), (2, 90, 0)\n",
    "    \n",
    "Entropy of Group 1 $=0$, Entropy of Group 2 $=1$\n",
    "\n",
    "- Split on 82\n",
    "    - Group 1: (1, 68, 0), (1, 82, 1), (2, 66, 0)\n",
    "    - Group 2: (2, 90, 0)\n",
    "    \n",
    "Entropy of Group 1 $=2.75$, Entropy of Group 2 $=0$\n",
    "\n",
    "- Split on 66\n",
    "    - Group 1: (2, 66, 0)\n",
    "    - Group 2: (1, 68, 0), (1, 82, 1), (2, 90, 0)\n",
    "\n",
    "Entropy of Group 1 $=0$, Entropy of Group 2 $=.9$\n",
    "\n",
    "- Split on 90\n",
    "    - Group 1: (2, 90, 0)\n",
    "    - Group 2: (1, 68, 0), (1, 82, 1), (2, 66, 0)\n",
    "   \n",
    "Entropy of Group 1 $=0$, Entropy of Group 2 $=.9$\n",
    "\n",
    "Therefore we can either split on weather or on a temperature of 66."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Splits\n",
    "This however is only a very basic tree. What about more complex trees? After choosing the best split the natural choice would then be to split recursively. For example, if we split on sunny\n",
    "\n",
    "- Split on Sunny\n",
    "    - Group 1: (1, 68, 0), (1, 82, 1)\n",
    "    - Group 2: (2, 66, 0), (2, 90, 0)\n",
    "\n",
    "We have already established that the entropy of group 2 is 0, so we would no longer split. But for group 1 the entropy is 2 so we would split again. Splitting on either 68 or 82 would yield trees with zero entropy so we could arrive at either of the following trees\n",
    "\n",
    "       is Sunny               is Sunny\n",
    "      N /    \\ Y             N /    \\ Y\n",
    "            68 <                   82 < \n",
    "         N /   \\ Y              N /   \\ Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets implement it\n",
    "def entropy(probs):\n",
    "    # probs = probabilities\n",
    "    epsilon = (1e-2)\n",
    "    return -np.dot(probs, np.log2(probs + epsilon))\n",
    "\n",
    "def partial_entropy(X, partial):\n",
    "    if len(X[partial]) == 0:\n",
    "        return 0\n",
    "    # calculate the entropy of one part of the tree (either Y or N)\n",
    "    positive_prob = (np.sum(X[:, -1][partial])/ sum(partial))\n",
    "    negative_prob = (1 - positive_prob)\n",
    "    return entropy(np.array([positive_prob, negative_prob]))\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, feature, split, class_=None):\n",
    "        # feature (the thing which this node is split on)\n",
    "        # split (the boundary at which this node is split)\n",
    "        # class_ (if this is a leaf node, what class does the elements belong to)\n",
    "        self.feature = feature\n",
    "        self.split = split\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.class_ = class_\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.class_ is not None:\n",
    "            return self.class_\n",
    "        \n",
    "        else:\n",
    "            x_feat_val = x[self.feature]\n",
    "\n",
    "            if x_feat_val < self.split:\n",
    "                return self.left.forward(x)\n",
    "            else:\n",
    "                return self.right.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X):\n",
    "    if X is None:\n",
    "        return\n",
    "    \n",
    "    if len(X) == 1:\n",
    "        return Node(feature=None, split=None, class_=X[0][-1])\n",
    "    \n",
    "    n_rows, n_features = X.shape\n",
    "    best_split = {'feature': 0, 'entropy' : float('inf'), 'left' : None, 'right' : None}\n",
    "    \n",
    "    for feature_num, feature_values in enumerate(X[:, :-1].T):\n",
    "        for split in np.unique(feature_values):\n",
    "            left = feature_values < split\n",
    "            left_entropy, right_entropy = partial_entropy(X, left), partial_entropy(X, ~left)\n",
    "            total_entropy =  left_entropy + right_entropy\n",
    "            \n",
    "            if total_entropy < best_split['entropy']:\n",
    "                best_split = {'feature': feature_num,\n",
    "                              'split_val' : split,\n",
    "                              'entropy' : total_entropy, \n",
    "                              'left' : X[left], \n",
    "                              'right' : X[~left],\n",
    "                              'split' : split\n",
    "                             } \n",
    "    \n",
    "    root = Node(feature=best_split['feature'], split=best_split['split_val'])\n",
    "    root.left = fit(best_split['left'])\n",
    "    root.right = fit(best_split['right'])\n",
    "    \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(decision_tree.forward([3.9, 3.9]), \n",
    "decision_tree.forward([3.2, 3.2]), \n",
    "decision_tree.forward([2.7, 2.7]),\n",
    "decision_tree.forward([2., 2.]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

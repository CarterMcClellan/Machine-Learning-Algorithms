{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    theta = np.array([3, 1, 3])\n",
    "    return 1/(1 + np.exp(-1*np.dot(x, theta)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Classification. Input set of features, map it to a discrete value, if binary classification, $y \\in \\{0, 1\\}$. Thus we want a function which neatly maps inputs to either $0$ or $1$. There are a couple candidates, $\\tanh$, $\\sigma$, $\\ldots$. Logistic regression uses the logistic function $\\sigma$\n",
    "\n",
    "$$h_\\theta(x) = \\sigma(\\theta_n \\cdot X) = \\frac{1}{1 + e^{-(\\theta_1 \\cdot x_1 + \\ldots \\theta_n \\cdot x_n)}}$$\n",
    "\n",
    "Note(s)\n",
    "$$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss \n",
    "Lets optimize our function such that \n",
    "\n",
    "$$P(y=1; x, \\theta) = h_\\theta(x)$$\n",
    "and \n",
    "$$P(y=0; x, \\theta) = 1 - h_\\theta(x)$$\n",
    "\n",
    "Now we want to maximize, the probability our classifier is right. So create a function which returns the probability our classifier assigned to the correct class label.\n",
    "\n",
    "$$P(y; x, \\theta) = h_\\theta(x)^y \\cdot (1 - h_\\theta(x))^{1-y}$$\n",
    "\n",
    "Eg, if the correct class label is 1, the second term is cancelled and we return $h_\\theta$ which the exactly the probability (accordiningly to our classifier) that $y = 1$, and this holds in reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Derivation\n",
    "The likelihood of $P(y;x, \\theta)$ over $n$ independent r.v's (our training set) would then be\n",
    "$$P(y; x, \\theta) = \\prod_{i=1}^n h_\\theta(x)^y \\cdot (1 - h_\\theta(x))^{1-y}$$\n",
    "\n",
    "We can see though that this is pretty hard to differentiate. Since we are dealing multiplication and exponentials and we would rather see addition and coefficients, and because log is differentiable, we can instead compute the log-likelihood given by\n",
    "\n",
    "$$J_\\theta = \\log P(y; x, \\theta) = \\log \\prod_{i=1}^n h_\\theta(x)^y \\cdot (1 - h_\\theta(x))^{1-y} =$$\n",
    "$$\\sum_{i=1}^n (y) \\cdot \\log(h_\\theta(x)) + (1- y) \\cdot \\log(1 - h_\\theta(x))$$\n",
    "\n",
    "Now we want to maximize the gradient, knowing that gradient ascent look something like\n",
    "$$\\theta_j := \\theta_j + \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j} J_\\theta$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j}\\sum_{i=1}^n (y) \\cdot \\log(h_\\theta(x)) + (1- y) \\cdot \\log(1 - h_\\theta(x)) = \n",
    "\\frac{y}{h_\\theta(x)} \\cdot \\frac{\\partial h_\\theta(x)}{\\partial \\theta_j} + \n",
    "\\frac{(1 -y)}{h_\\theta(x)} \\cdot \\frac{-\\partial h_\\theta(x)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "$$ = \\left(\\frac{y}{h_\\theta(x)} - \\frac{(1 -y)}{h_\\theta(x)}\\right) \\cdot \\frac{\\partial h_\\theta(x)}{\\partial \\theta_j}$$\n",
    "\n",
    "To make taking the partial derivative easier, we then want to subtitute $h_\\theta(x) = \\sigma(\\theta_n \\cdot X)$\n",
    "\n",
    "$$ = \\left(\\frac{y}{\\sigma(\\theta_n \\cdot X)} - \\frac{(1 -y)}{\\sigma(\\theta_n \\cdot X)}\\right) \\cdot \\frac{\\partial \\sigma(\\theta_n \\cdot X)}{\\partial \\theta_j} = \n",
    "\\left(\\frac{y}{\\sigma(\\theta_n \\cdot X)} - \\frac{(1 -y)}{\\sigma(\\theta_n \\cdot X)}\\right) \\cdot \\sigma(\\theta_n \\cdot X) \\cdot (1 - \\sigma(\\theta_n \\cdot X)) \\cdot \\frac{\\partial \\theta_n \\cdot X}{\\partial \\theta_j}$$\n",
    "\n",
    "Partially differentiating the last bit yields\n",
    "$$\\left(\\frac{y}{\\sigma(\\theta_n \\cdot X)} - \\frac{(1 -y)}{\\sigma(\\theta_n \\cdot X)}\\right) \\cdot \\sigma(\\theta_n \\cdot X) \\cdot (1 - \\sigma(\\theta_n \\cdot X)) \\cdot x_j$$\n",
    "\n",
    "Distributing terms gives us\n",
    "\n",
    "$$(y \\cdot (1 - \\sigma(\\theta_n \\cdot X)) - (1-y) \\cdot \\sigma(\\theta_n \\cdot X)) \\cdot x_j = \n",
    "(y - \\sigma(\\theta_n \\cdot X)) \\cdot x_j \n",
    "$$\n",
    "\n",
    "$$= (y - h_\\theta(x)) \\cdot x_j$$\n",
    "\n",
    "Which funnily enough is the same result we got when computing the loss of our previous $J_\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test it out\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_gradient_descent():\n",
    "    theta = np.zeros()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

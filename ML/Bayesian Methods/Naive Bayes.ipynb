{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports (Not Important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris()\n",
    "X, y = dataset['data'], dataset['target']\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Classification in the abstract looks to find the probability of a class given a feature vector\n",
    "$$p(\\text{Class} ~|~ (x_1, x_2, \\ldots, x_n) = \\frac{p((x_1, x_2, \\ldots, x_n), \\text{Class})}{p((x_1, x_2, \\ldots, x_n))}$$\n",
    "Note that the denominator is constant with the Class, and thus calculating it will not contribute to the classification, so it can be ignored! Thus we only need to find the numerator. By the chain rule the numerator can be rewritten as \n",
    "$$p((x_1, x_2, \\ldots, x_n), \\text{Class}) = p(x_1~|~x_2, \\ldots x_n, \\text{Class}) \\cdot p(x_2 ~|~ x_3 \\ldots, x_n, \\text{Class}) \\cdots p(x_n ~|~ \\text{Class}) \\cdot p(\\text{Class}) \\tag{1}$$\n",
    "\n",
    "Up to this point its all been just fact, but using our *Naive* assumption that all variables are indepedent, eq (1) becomes\n",
    "$$p(x_i ~|~ x_{i+1} \\ldots x_n, \\text{Class}) \\approx p(x_i ~|~ \\text{Class})$$\n",
    "So now\n",
    "$$\n",
    "p(\\text{Class} ~|~ (x_1, x_2, \\ldots, x_n) \\approx p((x_1, x_2, \\ldots, x_n), \\text{Class}) \\approx p(\\text{Class}) \\prod_{i=1}^n p(x_i ~|~ \\text{Class})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Points\n",
    "Want to find\n",
    "$$p(\\text{Class}) \\prod_{i=1}^n p(x_i ~|~ \\text{Class})$$\n",
    "Practically speaking we find those values simply by counting occurences in the dataset\n",
    "$$p(\\text{Class}) = \\frac{\\text{Count(Class)}}{\\text{Number of Datapoints}} \\quad p(x_i ~|~ \\text{Class}) = \\frac{\\text{Count(Class and $x_i$)}}{\\text{Count(Class)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_class_and_x(X, y, target_X, target_y):\n",
    "    select = (y == target_y) & (X == target_X)\n",
    "    return sum(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        N = len(y) # N = number of datapoints\n",
    "        _, n_col = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        self.count_class = dict(Counter(y.tolist()))\n",
    "        self.p_class = {_class : count/N for _class, count in self.count_class.items()}\n",
    "        self.count_values = {}\n",
    "        self.N = N\n",
    "        \n",
    "        # xi_vals[col][_class][val] = cnt\n",
    "        col_vals = {}\n",
    "        for col in range(n_col):\n",
    "            class_vals = {}\n",
    "            for _class in np.unique(y):\n",
    "                count_vals = {}\n",
    "                for val in np.unique(X[:, col]):\n",
    "                    cnt = count_class_and_x(X[:, col], y, val, _class)\n",
    "                    count_vals[val] = cnt\n",
    "                class_vals[_class] = count_vals\n",
    "            \n",
    "            col_vals[col] = class_vals\n",
    "        \n",
    "        self.count_values = col_vals\n",
    "            \n",
    "                        \n",
    "    def _compute_class_prob(self, _class, row):\n",
    "        class_prob = self.p_class[_class]\n",
    "        \n",
    "        for col, val in enumerate(row):\n",
    "            cnt = 0\n",
    "            if val in self.count_values[col][_class]:\n",
    "                cnt = self.count_values[col][_class][val]\n",
    "            \n",
    "            # LAPLACE SMOOTHING\n",
    "            # this is to handle the 0 count case\n",
    "            prob = (cnt + 1.)/(self.count_class[_class] + self.N)\n",
    "            class_prob *= prob\n",
    "        return class_prob\n",
    "    \n",
    "    def predict(self, row):\n",
    "        class_probs = [(_class, self._compute_class_prob(_class, row)) for _class in self.classes]\n",
    "        return max(class_probs, key=lambda x : x[1])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Test\n",
    "- Fit the Classifier on all the data, then feed it back the same data and see how well it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "NB = NaiveBayes()\n",
    "NB.fit(X, y)\n",
    "num_correct = 0\n",
    "for i in range(len(test_X)): \n",
    "    _guess, confidence = NB.predict(test_X[i])\n",
    "    if _guess == test_y[i]:\n",
    "        num_correct += 1\n",
    "print(\"Accuracy\", num_correct/len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generality Test\n",
    "- Testing with data the classifier has not seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "NB = NaiveBayes()\n",
    "NB.fit(train_X, train_y)\n",
    "num_correct = 0\n",
    "for i in range(len(test_X)): \n",
    "    _guess, confidence = NB.predict(test_X[i])\n",
    "    if _guess == test_y[i]:\n",
    "        num_correct += 1\n",
    "print(\"Accuracy\", num_correct/len(test_X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

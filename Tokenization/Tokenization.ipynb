{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start from the Start: Word Tokenization\n",
    "Tokenization: We have some sequence of characters which we need to translate into integer indices\n",
    "\n",
    "$$\\text{[the, cat, and, the, hat]} \\to [10, 14, 102, 10, 11]$$\n",
    "\n",
    "What are the major problems which we need to solve for with word tokenization\n",
    "- **Big Vocabulary** every word which does not have a corresponding token is an issue. unk or unknown tokens provide almost no information to the model and are bad. Stemming and Lemmatization are part of the way we solve for this\n",
    "- **Combined Words** certain words and phrases can be ambiguous in how they should be tokenized: \"sunflower\", \"B.S\" (bachelors of science), \"New York\"\n",
    "- **Abbreviations** should we expand IMO, LMAO\n",
    "- **Splitting** some languages dont segement by spaces, eg korean\n",
    "\n",
    "Why limit the size of a vocabulary at all?\n",
    "- \"Note that the cost of training a neural network scales with the size of its vocabulary. (this is what word2vec is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about Character Tokenization\n",
    "To avoid the problems list above, what if we tokenized a sentence using characters\n",
    "$$\\text{[t, h, e,  ,]} \\to [116, 104, 101, 32]$$\n",
    "\n",
    "What are the drawbacks of characer level tokenization\n",
    "- **Lack of Meaning**, as invidual units, characters do not hold as much meaning\n",
    "- **Input Length**, the above sentence \"the cat and the hat\" suddenly goes from being 5 tokens to 19\n",
    "- **Attention**, increasing the size of your input vector can also affect your architecture. It can take longer to input larger sequences, and it can be hard for RNN architectures to hold information from the begining of the sequence. \n",
    "\n",
    "Given that input length and attention are both problems solved by transformers, it makes sense that character level encoding can be more efficient with transformer architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenization\n",
    "Another way to handle the Big Vocabulary issue of Word Tokenization, is by reusing tokens from word parts. Simple examples being \"sun\" and \"flower\" rather than creating an all new token \"sunflower\", but we can also create parts which aren't words themselves, for example \"unfortunate\" could become \"un\", \"for\", \"tun\", \"ate\", thought typically most word parts will be smaller words.\n",
    "\n",
    "Typically speaking in such algorithms, the granularity of such word parts is dependent on the vocabulary size alotted, the larger tha vocabulary size, the larger the word parts.\n",
    "\n",
    "There are several subword tokenization algorithms\n",
    "- **BPE** or Byte Pair Encoding\n",
    "- **WordPiece**\n",
    "- **Unigram**\n",
    "- **Sentpiece**\n",
    "\n",
    "These all get their own notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Tokenizers: How Machines Read](https://blog.floydhub.com/tokenization-nlp)\n",
    "- [Lei Mao: Byte Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding)\n",
    "- [A great example with actual code](https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html#12_probablistic_subword_segmentation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

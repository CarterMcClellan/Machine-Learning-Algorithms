{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE (Byte Pair Encoding)\n",
    "In this jupyter notebook we are going to walk through the Byte Pair Encoding tokenization algorithm. Byte Pair Encoding is the first of several subword tokenization algorithms. At a very high level it works in 3 steps\n",
    "1. Compute the frequency of all token pairings\n",
    "2. Choose the most frequent token pairing (t1, t2)\n",
    "3. Merge t1 and t2, treating them as a single token within our vocabulary\n",
    "\n",
    "Read the original paper: [this paper](https://arxiv.org/pdf/1508.07909.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "Note that throughout this document I will use the following terms\n",
    "- **Corpus** \n",
    "    - a raw unformatted text blob\n",
    "    - eg. \n",
    "    ```\n",
    "    \"CHAPTER I\n",
    "        \"Well, Prince, so Genoa and Lucca are now just family estates of the\n",
    "        Buonapartes. But I warn you, if you don't tell me that this means war,\n",
    "        if you still try to defend the infamies and horrors perpetrated by that\n",
    "        Antichrist--I really believe he is Antichrist--I will have nothing more\n",
    "        to do with you and you are no longer my friend, no longer my 'faithful\n",
    "        slave,' as you call yourself! But how do you do? I see I have frightened\n",
    "        you--sit down and tell me all the news.\"\n",
    "    ```\n",
    "- **Document** \n",
    "    - a list of strings (input to BPE)\n",
    "    - eg. \n",
    "    ```python\n",
    "    ['\"Well, Prince, so Genoa and Lucca are now just family estates of the',\n",
    "     \"Buonapartes. But I warn you, if you don't tell me that this means war,\",\n",
    "     'if you still try to defend the infamies and horrors perpetrated by that',\n",
    "     'Antichrist--I really believe he is Antichrist--I will have nothing more',\n",
    "     \"to do with you and you are no longer my friend, no longer my 'faithful\",\n",
    "     \"slave,' as you call yourself! But how do you do? I see I have frightened\",\n",
    "     'you--sit down and tell me all the news.\"']\n",
    "    ```\n",
    "- **Vocabulary**\n",
    "    - a dictionary which holds our set of tokens and their frequencies\n",
    "    - eg\n",
    "       ```bash\n",
    "            a n d </w> : 34\n",
    "            t h e </w> : 29\n",
    "            a </w> : 29\n",
    "        ```\n",
    "- **Token** \n",
    "    - an element in our vocabulary.\n",
    "    - eg. if our vocabulary is\n",
    "        ```bash\n",
    "            a n d </w> : 34\n",
    "            t h e </w> : 29\n",
    "            a </w> : 29\n",
    "        ```\n",
    "    then our tokens are\n",
    "        ```bash\n",
    "            a n d </w>\n",
    "            t h e </w>\n",
    "            a </w>\n",
    "        ```\n",
    "    \n",
    "- **Character** \n",
    "    - a single element in a string \"a\" is a character \"at\" is not a character\n",
    "    - eg if the input strings are\n",
    "    ```\n",
    "        \"abc\"\n",
    "    ```\n",
    "    - then the characters are\n",
    "    ```\n",
    "        \"a\", \"b\", \"c\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, re, json, string\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To demonstrate how this tokenization algorithm works we first need a sample corpus. I pulled War and Peace as a raw text file from [here](https://raw.githubusercontent.com/mmcky/nyu-econ-370/master/notebooks/data/book-war-and-peace.txt), and I've implemented several simple rules to ensure that the the sample sentences which we pull are interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_document(corpus):\n",
    "    \"\"\" returns list of strings (sentences from war and peace) \"\"\"\n",
    "    example_sentences = []\n",
    "    MIN_NUM_CHARACTERS, SAMPLE_PROB, CAPACITY = 5, .1, 50\n",
    "    with open(corpus, \"r\") as f:\n",
    "        for sentence in f.readlines():\n",
    "            # strip whitespace, remove all new lines\n",
    "            sentence = sentence.strip().replace(\"\\n\", \"\").lower()\n",
    "            sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "            # all sentences myst have at least \n",
    "            # {MIN_NUM_CHARACTERS}\n",
    "            if len(sentence) > MIN_NUM_CHARACTERS:\n",
    "                \n",
    "                # sample sentences with probability\n",
    "                # {SAMPLE_PROB}\n",
    "                if random.random() < SAMPLE_PROB:\n",
    "                    example_sentences.append(sentence)\n",
    "            \n",
    "            # we never want to sample more than\n",
    "            # {CAPACITY} sentences from our corpus\n",
    "            if len(example_sentences) >= CAPACITY:\n",
    "                break\n",
    "    return example_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = os.path.join(Path(os.getcwd()).parent.parent, \"Datasets\", \"war_and_peace.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_document = get_example_document(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def print_helper(vocab, N):\n",
    "    \"\"\"prints the top {N} tokens and their counts in the given vocabulary\"\"\"\n",
    "    sorted_by_count = sorted(list(vocab.items()), key=lambda x: x[1], reverse=True)[:N]\n",
    "    for key, val in sorted_by_count:\n",
    "        print(\"{} : {}\".format(key, val))\n",
    "    \n",
    "    if N > len(vocab):\n",
    "        print(\"\\tetc...\")\n",
    "        \n",
    "def get_tokens(vocab):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        eg. {}\n",
    "    \"\"\"\n",
    "    tokens = defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens, vocab_tokenization\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Generate Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(document):\n",
    "    \"\"\"\n",
    "    split every element in the document\n",
    "    by whitespace \" \"\n",
    "        eg. \"the cat in the hat\" -> [the, cat, in, the, hat]\n",
    "    \n",
    "    split each word into a list of characters,\n",
    "    add a whitespace on the end, and the string '</w>'\n",
    "    to denote the end of a word \n",
    "        eg [the, cat, in, the, hat] -> ['t h e </w>', 'c a t </w>', 'i n </w>', 'h a t </w>']\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    for sentence in document:\n",
    "        word_tokens = sentence.strip().split()\n",
    "        for token in word_tokens:\n",
    "            vocab[' '.join(list(token)) + ' </w>'] += 1\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t h e </w> : 26\n",
      "a n d </w> : 15\n",
      "o f </w> : 15\n",
      "t o </w> : 12\n",
      "a </w> : 12\n",
      "i n </w> : 11\n",
      "t h a t </w> : 9\n",
      "i </w> : 8\n",
      "y o u </w> : 8\n",
      "a n n a </w> : 8\n"
     ]
    }
   ],
   "source": [
    "starting_vocab = get_vocab(example_document)\n",
    "print_helper(starting_vocab, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Count Character Pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_token_pairs(vocab):\n",
    "    \"\"\"\n",
    "    iterate through vocab tokens character by character\n",
    "    finding the most common adjacencies\n",
    "    \n",
    "    eg.\n",
    "        vocab = {'c a t </w>' : 10, 'h a t </w>' : 2}\n",
    "    \n",
    "        most_frequent_token_pairs(vocab) = \n",
    "            {('c', 'a'): 10, ('a', 't'): 12, ('t', '</w>'): 12, ('h', 'a'): 2})\n",
    "            \n",
    "    note that the frequencies of given tokens are added onto the pairs\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for token, freq in vocab.items():\n",
    "        characters = token.split()\n",
    "        for i in range(len(characters)-1):\n",
    "            pairs[characters[i],characters[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', '</w>') : 104\n",
      "('d', '</w>') : 71\n",
      "('t', 'h') : 64\n",
      "('h', 'e') : 62\n",
      "('t', '</w>') : 48\n",
      "('i', 'n') : 47\n",
      "('e', 'r') : 45\n",
      "('s', '</w>') : 43\n",
      "('h', 'a') : 39\n",
      "('a', 'n') : 38\n"
     ]
    }
   ],
   "source": [
    "starting_vocab = get_vocab(example_document)\n",
    "character_pair_frequency = most_frequent_token_pairs(starting_vocab)\n",
    "print_helper(character_pair_frequency, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Merging Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"\n",
    "    BPE starts by represents all characters in a word as\n",
    "    standalone characters (note how they are delimited by \n",
    "    spaces)\n",
    "    \n",
    "    eg. t h e </w>\n",
    "    \n",
    "    if we had an input pair (e, </w>), meaning that the two\n",
    "    characters which most often follower each other were \"e\"\n",
    "    followed by the endword token \"</w>\" then we would want \n",
    "    to merge these two tokens into one single token \"e</w>\"\n",
    "    \n",
    "    eg. t h e </w> -(merge operation)-> t h e</w>\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t h e</w> : 26\n",
      "a n d </w> : 15\n",
      "o f </w> : 15\n",
      "t o </w> : 12\n",
      "a </w> : 12\n",
      "i n </w> : 11\n",
      "t h a t </w> : 9\n",
      "i </w> : 8\n",
      "y o u </w> : 8\n",
      "a n n a </w> : 8\n"
     ]
    }
   ],
   "source": [
    "pair, count = max(character_pair_frequency.items(), key=lambda x: x[1])\n",
    "new_vocab = merge_vocab(pair, starting_vocab)\n",
    "print_helper(new_vocab, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(vocab, num_merges):\n",
    "    \"\"\" putting together all the steps to create BPE\"\"\"\n",
    "    most_common_pairings = []\n",
    "    vocab_lengths = []\n",
    "    \n",
    "    for _ in range(num_merges):\n",
    "        \n",
    "        # 1.Compute the frequency of all token pairings\n",
    "        token_pairings = most_frequent_token_pairs(vocab)\n",
    "        \n",
    "        # edge case:\n",
    "        #   if there is no whitespaces in the tokens,\n",
    "        #   most_frequent_token_pairs returns an empty dict\n",
    "        if token_pairings:\n",
    "            # 2. Choose the most frequent token pairing (t1, t2)\n",
    "            most_common_pairing, _ = max(token_pairings.items(), key=lambda x: x[1])\n",
    "            most_common_pairings.append(most_common_pairing)\n",
    "\n",
    "            # 3. Merge t1 and t2, treating them as a single token within our vocabulary\n",
    "            vocab = merge_vocab(most_common_pairing, vocab)\n",
    "            tokens, _ = get_tokens(vocab)\n",
    "            vocab_lengths.append(len(tokens))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return vocab, most_common_pairings, vocab_lengths  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_vocab = get_vocab(example_document)\n",
    "final_vocab, most_common_pairings, vocab_lengths = bpe(initial_vocab, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Performance\n",
    "How do we expect BPE to perform? We know that the intial vocab size is going to increase as we merge unigrams into bigrams (there are going to be more unigrams than bigrams). We also expect that after a certain number of merges our vocab will decrease as n-grams become words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb426f2e910>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAArrElEQVR4nO3deXxU1fnH8c+TkIR9X2SPIKuIKGFxqfuuLdbdKihF0Z+22tbWqm21rXZzt79aFUEBEZHWtcpPUcS1yr4T2WTfwr4Esj+/P+YmBgxhgExuJvN9v17zmrnn3jt5TgjzzLnn3HPM3REREQFICjsAERGpOpQURESkhJKCiIiUUFIQEZESSgoiIlKiRtgBHImmTZt6enp62GGIiMSVGTNmbHb3ZmXti+ukkJ6ezvTp08MOQ0QkrpjZygPt0+UjEREpoaQgIiIllBRERKSEkoKIiJRQUhARkRJKCiIiUkJJQURESigpiIjEEXfnqQ+XsHDdzpi8f1zfvCYikghyCwpZtGEXL3+1ii+WbWbNtr3kFBTSvVX9Cv9ZSgoiIlXMazPWkLn+25bAZ0s2s2jjLgDO7d6Ca/u2439O7xiTn62kICJSCTbuzGH55mxqJBltGtUmKQmWb8qmSd1U2jSqzavTVjPi8+XszS9k065caqYkkWwGQJ20Gjx0aQ96tmlAzzYNYxqnkoKISAzl5Bfy1KQlvPD5cnILiso8pm5aDXbnFtClRT1OOaYJ7RrXYehpHUhOskqOVklBRCQmtuzO5ebR08lcv4u9+YX0bNOAu87rwpKNu/jzhEwa1k7lvou68eniTcxctY1fnteZa/q2o2ZKcqhxm7uHGsCRyMjIcM2SKiJVzesz1/DYxMVs3p3LtX3b0b9DEy7ocVTYYZUwsxnunlHWPrUUREQqwIrN2SxYt5MlWbt48sMlNK2bxrMDe3Nml+Zhh3ZIlBRERI5ATn4hf5mQydipq8gvjFx5Oatrc54b2JuU5Pi7FUxJQUTkEOQWFDIpM4s9eYUAvDt3HZMXbaJ/h8bce2E36qQl06FpXZJC6CSuCEoKIiLlyMkvJDnJSDZj4sINPPzeIr7ZnF2y3wz+OOBYBp2UHl6QFUhJQUSkDDtz8hnx2XKe/WQZ9WulUC+tBt9szqZJnVQeurQHp3eOLHFcKzWZpnXTQo624igpiEjC2pqdx5fLtjD6yxUUFDl5BUXUTIn0A6zbnsPa7XtJb1KbnTkFrNm+lzvO7sRN3zua+jVTQo48dpQURCShzFy1jWnLt7Jy6x7GTlkFQPsmtVm7bS8FRc5JHZpgBh2a1eE3F3fjgmOPIr+oiIJCp05a9f/IrP41FBEBduXk86t/zeW9BRtKyr5/fCvO7NKM8449im3ZeezYm0+P1g2+c25aUjIJkA8AJQURqWYKi5xx01aRtTN3n/LPlmxi1urtXNqrFfdd1I16NVOolfrt3cN102rQtrKDrYKUFEQk7mXnFgAwf+0OfjF+Dmu37/3OMbVSknny6l4M6NW6ssOLK0oKIhK33J1Xpq7miQ8Xk5NfSF5BEfVqpvDgpT0Y2L992OHFJSUFEanS3J2H3s3k3bnrAbi4Z0vSm9Rm2GffkJNfxKZd314m6tayPi/e2IejGtQMK9y4p6QgIlXSpMyNTF6UxbrtOXz0dRbndGtOkcOIz5cD0KN1fU7u0IBOLeoy5NSjMYvPO4irmpglBTOrCXwKpAU/59/u/oCZHQ2MAxoDM4GB7p5nZmnAaKA3sAW42t1XxCo+Eala8guLmLZiK58s3sSr01azY28+dVNrkJaSxG1ndORX53cBYO6aHezJK+TE9g1JqxHuNNPVUSxbCrnAWe6+28xSgM/N7P+AXwBPuPs4M3sWGAI8Ezxvc/djzOwa4G/A1TGMT0SqgK3Zefx90hLmrNnOrFXbATizSzN6tW3E0NM67DNCCOD4tg0rP8gEErOk4JGFGnYHmynBw4GzgB8F5aOA3xNJCgOC1wD/Bv5hZubxvOCDiHxHdm4BXyzdTHrTOoyftprhny8nJdlo1bAWv7moG6cc05RuLevpclBIYtqnYGbJwAzgGOBpYBmw3d0LgkPWAMXjw1oDqwHcvcDMdgBNgM37vedQYChAu3btYhm+iFSg/MIi/jNnHY9NXLzPkNE+6Y2458Ku9G7fOMTopFhMk4K7FwK9zKwh8AbQrazDgueyvhZ8p5Xg7sOAYRBZea1iIhWRWMjJL+TDzI18tngzr05fDUCL+mncenpHFq7fyeCT0zmjSzO1CqqQShl95O7bzexjoD/Q0MxqBK2FNsC64LA1QFtgjZnVABoAWysjPhGpWBt35vCndzNZuH4nS7MiV5EvO6E1J7RryFV92qqDuAqL5eijZkB+kBBqAecQ6TyeDFxBZATSDcBbwSlvB9tfBvs/Un+CSNXn7nyYmcWGnTklZaP/u4I12/bStWU9HrmiJ6d1bkaL+rp3IB7EsqXQEhgV9CskAePd/R0zWwiMM7OHgFnAiOD4EcBLZraUSAvhmhjGJiJHKL+wiIfeWcis1duZu2bHPvtqpybzwo19OKljk5Cik8MVy9FHc4ETyij/BuhbRnkOcGWs4hGRilNY5Nw1fg5vz1lHn/RG3HHWMVx/Unss6Bqsk5ZM7VTdGxuP9K8mIlHJyS9k3fa9rNiSzUPvZPLN5mx+fUFX/ueMjmGHJhVISUFE9vHBwo0sWLeDGknG5b3b0LJBLTbuzOGq575k5ZY9ADSuk8r9l3Tnx6ceHXK0UtGUFEQEgL15hbzwxXIeeX9RSdmwT7+hab00tmbnkV9QxJ9/eBx1a9bge8c0pVGd1BCjlVhRUhBJQMVTTq/ckk1ajSRqpibz0pcrWb8jhzO6NOO5gb1ZsnE3Iz5fTn5hEclJxqCT2usGswSgpCCSYLZm5/HoxEWMnbKKtBpJ5BYUAdCgVgp/uew4LjuxNWk1kunRugFPXN0r3GCl0ikpiCSIL5dtYcK89bw7bz1bs/O4rl87Hrq0B7NWb2fGim0MOfVokpJ0Z3GiU1IQqYb25hUyfeVWihxSkoy8wiKGjp5BjWSjXePaPHJFT87s0hwz48R2jTixXaOwQ5YqQklBpJrJzi3guuFTmL16+z7lXVrU49Vb+tOwtjqI5cCUFESqkZz8Qoa+NJ15a3fw18uOo1OLesxds50PFm7kyat7KSHIQSkpiFQTBYVF3PHKLL5YuoXHrjyey3u3AaB3+0YMPkX3E0h0lBRE4lRhkfPR11nsyskH4KOvs5i4cCMPfL97SUIQOVRKCiJxZHduARPmrWdCMIJo/4nofn5OZ7UK5IgoKYjEAXfn928vYNSXKwFIb1KbBrVSuPfCrlzQ4ygAaqYka3pqOWJKCiJx4G/vLWLUlyu5oncb+qY3ZsAJrbRQjcSEkoJIFffPj5fy7CfLuL5/Ox4c0ENLV0pMKSmIVEE7c/LZsjuPyV9n8fB7ixjQqxV//IESgsSekoJIFTNz1TYGDp9Cdl4hAGd3bc6jVx6vKSikUigpiFQhX2/YyeAXp9G0XhoPnt2J2qnJnNm1OSnJSWGHJglCSUGkilixOZuBI6ZSMyWJMUP60bZx7bBDkgSkrx8iVcCGHTlcP2IKBYVFSggSKrUUREK0ZOMuxk5dxSeLNrF9Tz5jb+5Hpxb1wg5LEpiSgkhIlm3azTXDvmJXbgEt6qfx/KAMerZpGHZYkuCUFERCsHb7XgYOn4IZvHfn9+jQrG7YIYkASgoilW7z7lwGDp/CrtwCxg3tr4QgVUrMkoKZtQVGA0cBRcAwd3/KzH4P3AxsCg69z90nBOfcCwwBCoE73P39WMUnUpkKCot48sMlLN+czcL1O1m/Yy9jhvTj2FYNwg5NZB+xbCkUAHe5+0wzqwfMMLMPgn1PuPujpQ82s+7ANcCxQCvgQzPr7O6FMYxRJOaKipy7X5vL6zPX0qFZHWqnJjNsYAYZ6Y3DDk3kO2KWFNx9PbA+eL3LzDKB1uWcMgAY5+65wHIzWwr0Bb6MVYwiFc3d2Z1bULK9aMMuHnxnIXPW7OAX53bmjrM7hRidyMFVSp+CmaUDJwBTgFOAn5jZIGA6kdbENiIJ46tSp62hjCRiZkOBoQDt2rWLbeAihyCyFOYMPl28aZ/y2qnJ/Or8Ltx2RseQIhOJXsyTgpnVBV4DfubuO83sGeBBwIPnx4AfA2VN7OLfKXAfBgwDyMjI+M5+kTBMytzI7/+zgNVb93Lr6R1pWjeyFnKSGZf0bElzrXMgceKgScHMrgTeCy4B/RY4EXjI3WdGcW4KkYTwsru/DuDuG0vtfx54J9hcA7QtdXobYF20FREJyyeLN3HrmBk0qp3K41cdz2UnailMiV/RtBR+5+7/MrNTgfOBR4FngH7lnWSROX5HAJnu/nip8pZBfwPAD4H5weu3gbFm9jiRjuZOwNRDqYxIZZm9ejuPf7CY7XvyWLRhF52a1+OVof1pUCsl7NBEjkg0SaF49M/FwDPu/lYwrPRgTgEGAvPMbHZQdh9wrZn1InJpaAVwC4C7LzCz8cBCIiOXbtfII6lq3J2H3s1kxOfLqV+zBr3bN+Li41py70XdlBCkWogmKaw1s+eAc4C/mVkaUUyk5+6fU3Y/wYRyzvkT8KcoYhKpVO7O0qzdvDl7LSM+X84Pjm/FL8/rQrsmmrhOqpdoksJVwAXAo+6+3cxaAr+KbVgiVUdRkfOL8bN5c3aki+vK3m342+U9teiNVEsHTQruvsfM3gJamFnxGNCvYxuWSNXg7tz/9nzenL2OW07rwKmdmnJyx6ZKCFJtRTP66KfAA8BGItNVQKQ/oGcM4xKpEh6duIgxX63iltM7cO+F3cIORyTmorl8dCfQxd23xDoYkapiW3YeT09eyvDPl3Nt33bcc0HXsEMSqRTRJIXVwI5YByJSVazdvpcrn/kv63bkcEnPljx0aQ8iI6xFqr9oksI3wMdm9i6QW1xY+t4Dkepi065vp7V+9voTObf7USSr/0ASSDRJYVXwSA0eItXSjr35DHphKuuCaa01i6kkomhGH/0BwMzquHt27EMSqXx78woZMnIaS7N2MfyGPkoIkrAOehOamZ1kZguBzGD7eDP7Z8wjE6kk7y/YwIVPfcrMVdt46poTOL1zs7BDEgnNQZMC8CSROY+2ALj7HOC0GMYkUmkmLtjAbS/PZHduAY9f1YuLjmsZdkgioYpq6mx3X73f6AvNSSRx77/LNvOTV2bRo1V9Xr65P3XTtGS5SFRDUs3sZMDNLBW4g+BSkki8mr16OzePmk56k9qMHNxXCUEkEM3lo1uB24msgrYG6AXcFsOYRGJq8cZd3PjiVBrXTeWlIf1oVEeD6kSKRfP1qI+7X1e6wMxuBZ6NTUgisbN66x4GjphCanISLw/pTwutiCayj2haCr8zs7OKN8zsbmBA7EISiY2snTlcN3wKOflFvDSkn6a9FilDNC2FHwDvmNmviEyh3TUoE4kb2/fkMXDEVLbszuXlm/vT5ah6YYckUiVFc/PaZjP7AfAhMAO4wt095pGJVIBdOfm8+MUKnv1kGQVFzsgb+9CrbcOwwxKpsg6YFMxsF5EpsoulAh2AK8zM3b1+rIMTORLrd+zlime+ZO32vXRvWZ/fXNyNk49pGnZYIlXaAZOCu6t9LXFp/tod3DFuFuu351AjyXjy6l5c0rMlNZKj6UITSWxRDc4OLh8V38X8sbu/E7uQRA5PQWER//x4GcM+/YYGtVK4MqMNV/RuQ882DcMOTSRuRLPy2l+BPsDLQdGdZnaqu98T08hEDsHqrXv407uZvLdgA51b1OW5gRkc3bRO2GGJxJ1oWgoXAb3cvQjAzEYBswAlBQldTn4hD7+3iDFfrSSvsIg7zu7EL87tHHZYInEr2nv7GwJbg9cNYhOKyKGZMG89Pxk7kyKHvumNufeirpzQrlHYYYnEtfJGH0109/OAvwCzzGwyYET6Fu6tpPhEvsPd+TAzizvHzaJT83rcc1FXzuzSPOywRKqF8loKzQDc/RUz+5hIv4IBv3b3DQd7YzNrC4wGjgKKgGHu/pSZNQZeBdKBFcBV7r7NItOwPkXkctUe4EZ3n3mY9ZJqam9eITeNnsYXS7fQvWV9Xhnanwa1UsIOS6TaKC8pNDCzy8ooP9nMcPfXD/LeBcBd7j7TzOoBM8zsA+BGYJK7/9XM7iHSN/Fr4EKgU/DoBzwTPIsAkcVwHnxnIWu37+WW0zpw6+kdlRBEKli5SQG4hEjrYH8OlJsU3H09sD54vcvMMonMtDoAOCM4bBTwMZGkMAAYHdwt/ZWZNTSzlsH7SIKblLmR216eSdO6qTx5dS8G9Goddkgi1VJ5SWGlu/+4In6ImaUDJwBTgBbFH/Tuvt7Mii8GtwZWlzptTVC2T1Iws6HAUIB27dpVRHhShRUVORMXbuDOcbM5tlV9xmoxHJGYKu9/V1kthENmZnWB14CfufvO/VZwO9jP+84cS+4+DBgGkJGRoTmYqrHcgkJuGjWdz5ZsplPzuloMR6QSlPc/bOCRvrmZpRBJCC+X6oPYWHxZyMxaAllB+RqgbanT2wDrjjQGiU8FhUXc8cosPluymdvO6MjN3+ugxXBEKkF5cx/NP5I3DkYTjQAy3f3xUrveBm4A/ho8v1Wq/CdmNo5IB/MO9Scknl05+bz01UqmLd/K5EWbeOD73Rl8ytFhhyWSMGLZFj+FSGtjnpnNDsruI5IMxpvZEGAVcGWwbwKR4ahLiQxJHRzD2KQKysmPXC6asnwraTWS+NX5XZQQRCpZeTevTXL3s83sb+7+60N9Y3f/nAP3S5xdxvFOZC1oSUD5hUXc/vJMpq7YylPXaHSRSFjKaym0NLPTgR8El3T2+YDXjWVSUYqKnF/+aw6Tvs7iTz/soYQgEqLyksL9RG4sawM8vt8+B876zhkih8jduf/t+bw1ex13X9CF6/q1DzskkYRWXkfzv4F/m9nv3P3BSoxJEsijExcx5qtV3Hp6R24745iwwxFJeNGs0fygFtmRWHjuk2U8PXkZP+rXjl9f0CXscEQEOOj6hGb2F+BOYGHwuDMoEzlsr0xdxV/+72su6dmSBwf0oJybGkWkEkUzJPViyl5kR9Nny2H5z5x13PfGPM7s0ozHr+pFcpISgkhVEe1K5g1LvdYiO3LYJi/K4uevzqZP+8b887repNaI9k9QRCpDNC0FLbIjFWLq8q38z5gZdG1Zj+E3ZlArNTnskERkP9F0NB/WIjsipc1fu4MhI6fRumEtRg3uS/2aWgdBpCqKapqLYA6it2Mci1RTyzbt5oYXplK/VgovDelHk7ppYYckIgegC7oSU2u372Xg8CmYGWNu6kerhrXCDklEyqGkIDGzaVcu1w+fwu7cAkb/uC9HN60TdkgichDlJgUzSzKzI5pCWxLTjr35DHphKht25PDi4D50b1U/7JBEJArlJoXg3oQ5ZqZ1LyVqe/IKGDJyGkuzdvHcwN70bt847JBEJErRdDS3BBaY2VQgu7jQ3X8Qs6gkbuUVFHHrmJnMXLWNp390Iqd1bhZ2SCJyCKJJCn+IeRRSLRQWOT9/dTafLt7Ew5f35MLjWoYdkogcomjuU/jEzNoDndz9QzOrDeiuI9mHu3Pf6/N4d956fntxN67q0/bgJ4lIlRPNhHg3A/8GnguKWgNvxjAmiTPuzp8nZPLq9NXccdYx3PS9DmGHJCKHKZohqbcTWW95J4C7LwGaxzIoiS9PT17K858t58aT0/n5uZ3DDkdEjkA0SSHX3fOKN8ysBpGV10QY9d8VPDpxMZed2Jr7L+muKbBF4lw0SeETM7sPqGVm5wL/Av4T27AkHrwxaw0PvL2Ac7u34OHLe5KkKbBF4l40SeEeYBMwD7gFmAD8NpZBSdX3wcKN/PJfczm5YxP+99oTqJGsm+NFqoNoRh8VBQvrTCFy2WiRu+vyUQL777LN3D52Jj1aN2DYoAxqpmgwmkh1Ec3oo4uBZcDfgX8AS83swijOe8HMskpPk2FmvzeztWY2O3hcVGrfvWa21MwWmdn5h1cdiaWc/EJGfL6cm0ZNJ71JbUYN7kPdtKgm2hWROBHN/+jHgDPdfSmAmXUE3gX+7yDnjSSSREbvV/6Euz9ausDMugPXAMcCrYAPzayzuxdGEZ9UgoLCIn76yiw+WLiRDk3r8NKQfjSsnRp2WCJSwaJJClnFCSHwDZB1sJPc/VMzS48yjgHAOHfPBZab2VKgL/BllOdLDBUVOXf/ey4fLNzIby7qxuBT0tWHIFJNHTApmNllwcsFZjYBGE+kT+FKYNoR/MyfmNkgYDpwl7tvI3JD3FeljlkTlJUV11BgKEC7dpqnL9bcnT++s5DXZ63lrnM7c/NpujFNpDor7+ve94NHTWAjcDpwBpGRSI0O8+c9A3QEegHriVyagsgyn/srszPb3Ye5e4a7ZzRrpsnWYu2JDxYz8r8ruOnUo/nJWceEHY6IxNgBWwruPriif5i7byx+bWbPA+8Em2uA0pPltAHWVfTPl0Mz/LNv+PtHS7kqow2/ubibbkwTSQAH7VMws6OBnwLppY8/nKmzzaxlsN4zwA+B4pFJbwNjzexxIh3NnYCph/r+UnHGT1vNQ+9mcmGPo/jLZT2VEEQSRDQdzW8CI4jcxVwU7Rub2StELjc1NbM1wAPAGWbWi8iloRVEbobD3ReY2XhgIVAA3K6RR+HYsjuXW8fMYNqKbXyvU1OevKYXybpTWSRh2MHuQzOzKe7er5LiOSQZGRk+ffr0sMOoNnbm5HPtsK9YsnE3A09qz13ndaZ2qu5DEKluzGyGu2eUtS+a//FPmdkDwEQgt7jQ3WdWUHxSBezNK+SmkdNZtGEXzw/K4MyumghXJBFFkxSOAwYCZ/Ht5SMPtqUamPx1Fn+ekMnSTbv5+zUnKCGIJLBoksIPgQ6lp8+W6iE7t4AxX63kkfcXUSs1mYcv78n3j28VdlgiEqJoksIcoCFR3MUs8WNPXgEDR0xh5qrtHNe6AWNv7ke9milhhyUiIYsmKbQAvjazaezbp3DIQ1KlasgtKOSWl2Ywe/V2/vTDHlyV0ZYUTVshIkSXFB6IeRRSaQoKi7jzldl8tmQzj1zRkysz2h78JBFJGNGsp/BJZQQisVdU5Nz7+jzeW7CB313SXQlBRL4jmjuad/HtPESpQAqQ7e71YxmYVCx356F3M/nXjDXceXYnhpx6dNghiUgVFE1LoV7pbTO7lMi01hJH/j5pKS98sZwbT07nZ+d0CjscEamiDrl30d3fRPcoxJUXv1jOEx8u5vIT23D/Jd01j5GIHFA0l48uK7WZBGRwgGmtpep5bcYa/vCfhZx/bAv+dvlxJGkeIxEpRzSjj75f6nUBkYnsBsQkGqlQ7y/YwN2vzeWUY5rw1DUnaLU0ETmoaPoUKnxdBYm9L5Zu5qdjZ3Fc6wYMG5hBzZTksEMSkThQ3nKc95dznrv7gzGIRyrArFXbuHn0dI5uWoeRg/tQJ00znYpIdMr7tMguo6wOMARoAigpVEFfb9jJjS9Oo1m9NF4a0peGtVPDDklE4kh5y3EWr5+MmdUD7gQGA+P4dm1lqUJWbslm4Iip1ExJYsyQfjSvXzPskEQkzpR7XcHMGgO/AK4DRgEnuvu2yghMDs2GHTlcP2IKBYVFjL/lJNo2rh12SCISh8rrU3gEuAwYBhzn7rsrLSo5JNuy8xg4YgrbsvMZe3M/OrWod/CTRETKUN4YxbuAVsBvgXVmtjN47DKznZUTnhzM7twCbnxxKiu37uH5QRn0bNMw7JBEJI6V16egQe1VXE5+ITeNmsb8dTt57vrenNSxSdghiUic0wd/nMovLOInY2cyZflWHr/qeM7p3iLskESkGlBSiENFRc7d/57Lh5lZ/HFADwb0ah12SCJSTSgpxBl35/f/WcAbs9byq/O7MLB/+7BDEpFqREkhzjw2cTGjv1zJLad14LYzOoYdjohUMzFLCmb2gpllmdn8UmWNzewDM1sSPDcKys3M/m5mS81srpmdGKu44tnzn37DPyYv5dq+bbnnwq6aAltEKlwsWwojgQv2K7sHmOTunYBJwTbAhUCn4DEUeCaGccWlcVNX8acJmVzcsyUPXXqcEoKIxETMkoK7fwps3a94AJE7owmeLy1VPtojvgIamlnLWMUWb96du55735jH6Z2b8cRVvUjWmggiEiOV3afQwt3XAwTPzYPy1sDqUsetCcq+w8yGmtl0M5u+adOmmAZbFXy8KIufvTqLjPaNePb63qTWUDeQiMROVfmEKeurb5mru7n7MHfPcPeMZs2axTiscE1bsZVbx8ygc4t6jLixD7VStSaCiMRWZSeFjcWXhYLnrKB8DdC21HFtgHWVHFuVsmDdDn48chqtGtRi1I/7Ur9mStghiUgCqOyk8DZwQ/D6BuCtUuWDglFI/YEdxZeZEtE3m3YzaMRU6qXV4KWb+tG0blrYIYlIgojZklxm9gpwBtDUzNYADwB/Bcab2RBgFXBlcPgE4CJgKbCHyLoNCWnt9r1cP3wKAGNu6kfrhrVCjkhEEknMkoK7X3uAXWeXcawDt8cqlnjg7ny+dDO/fXM+u3ILGDe0Px2a1Q07LBFJMFq8t4r458fLeOT9RdRMSeKlIf04tlWDsEMSkQSkpFAFvPTVSh55fxEXHXcUfxzQQ30IIhIaJYWQvTV7Lfe/NZ9zujXnqWtOICW5qowSFpFEpE+gEH24cCO/GD+H/kc34R8/OlEJQURCp5ZCCMZPW82/Z65h9urt9GhVn+dvyKBmim5ME5Hw6atpJRs/fTV3vzaXbdl5nH/sUYwc3Je6acrNIlI16NOoEr03fz33vDaX73VqyvAbMkirodaBiFQtailUks+WbOKOV2bTq21DnhvYWwlBRKokJYVKMGPlNoaOnkGHZnV48ca+1E5VA01EqiYlhRjLXL+TwS9OpUX9NEYP6UuD2prYTkSqLiWFGFqxOZuBI6ZSO7UGY27qR/N6NcMOSUSkXEoKMbJ+x16uGz6FInfG3NSXNo1qhx2SiMhBKSnEwNbsPAaOmMqOvfmMGtyXY5rXCzskEZGoKClUsF05+dzwwlRWb93DiBsyOK6NJrYTkfihpFCBcvILGTJqOpnrd/LM9SfSr0OTsEMSETkkGhtZQfILi7jt5ZlMW7GVJ6/uxVldW4QdkojIIVNLoQIUFjl3jZ/DR19n8dClPRjQq3XYIYmIHBYlhSPk7tz/1nzenrOOuy/ownX92ocdkojIYVNSOEKPvL+Il6es4tbTO3LbGceEHY6IyBFRUjgCz36yjH9+vIwf9WvHry/oEnY4IiJHTEnhMI2dsoq//t/XXNKzJQ8O6IGZhR2SiMgRU1I4DP+Zs47fvDmPM7s04/GrepGcpIQgItWDksIhmvx1Fj9/dTZ92jfmn9f1JrWGfoUiUn2Ecp+Cma0AdgGFQIG7Z5hZY+BVIB1YAVzl7tvCiO9Api7fyq1jZtC1ZT2G35hBrVStiSAi1UuYX3PPdPde7p4RbN8DTHL3TsCkYLvKmL92B0NGTqNNo1qMGtyX+jU1BbaIVD9V6drHAGBU8HoUcGl4oexradZuBr0wlfq1UnhpSD+a1E0LOyQRkZgIKyk4MNHMZpjZ0KCshbuvBwiem4cU2z7WbNvDwBFTSDJjzE39aNWwVtghiYjETFhzH53i7uvMrDnwgZl9He2JQRIZCtCuXbtYxQfApl25DBwxlezcAsYNPYmjm9aJ6c8TEQlbKC0Fd18XPGcBbwB9gY1m1hIgeM46wLnD3D3D3TOaNWsWsxh37M1n0AtT2bAjhxcH96F7q/ox+1kiIlVFpScFM6tjZvWKXwPnAfOBt4EbgsNuAN6q7NiK7ckr4Mcjp7E0axfPDexN7/aNwwpFRKRShXH5qAXwRnAHcA1grLu/Z2bTgPFmNgRYBVwZQmzkFRRx65iZzFq1jad/dCKndY5da0REpKqp9KTg7t8Ax5dRvgU4u7LjKa2wyPn5q7P5dPEmHr68Jxce1zLMcEREKl1VGpIaKnfnvtfn8e689fz24m5c1adt2CGJiFQ6JQUiCeHPEzJ5dfpq7jjrGG76XoewQxIRCYWSAvD05KU8/9lybjw5nZ+f2znscEREQpPwSWH0lyt4dOJiLjuhNfdf0l1TYItIQkvopPDGrDXc/9YCzu3egoev6EmSpsAWkQSXsEnhg4Ub+eW/5nJyxyb877UnUCM5YX8VIiIlEvKT8MtlW7h97Ex6tG7AsEEZ1EzRFNgiIpCgSaFp3VT6Hd2YUYP7UDctrOmfRESqnoT8ROzUoh4vDekXdhgiIlVOQrYURESkbEoKIiJSQklBRERKKCmIiEgJJQURESmhpCAiIiWUFEREpISSgoiIlDB3DzuGw2Zmm4CVh3l6U2BzBYYTD1TnxKA6J4YjqXN7dy9zreG4TgpHwsymu3tG2HFUJtU5MajOiSFWddblIxERKaGkICIiJRI5KQwLO4AQqM6JQXVODDGpc8L2KYiIyHclcktBRET2o6QgIiIlEjIpmNkFZrbIzJaa2T1hx1NRzOwFM8sys/mlyhqb2QdmtiR4bhSUm5n9PfgdzDWzE8OL/PCZWVszm2xmmWa2wMzuDMqrbb3NrKaZTTWzOUGd/xCUH21mU4I6v2pmqUF5WrC9NNifHmoFDpOZJZvZLDN7J9iu1vUFMLMVZjbPzGab2fSgLKZ/2wmXFMwsGXgauBDoDlxrZt3DjarCjAQu2K/sHmCSu3cCJgXbEKl/p+AxFHimkmKsaAXAXe7eDegP3B78e1bneucCZ7n78UAv4AIz6w/8DXgiqPM2YEhw/BBgm7sfAzwRHBeP7gQyS21X9/oWO9Pde5W6JyG2f9vunlAP4CTg/VLb9wL3hh1XBdYvHZhfansR0DJ43RJYFLx+Dri2rOPi+QG8BZybKPUGagMzgX5E7m6tEZSX/J0D7wMnBa9rBMdZ2LEfYj3bBB+AZwHvAFad61uq3iuApvuVxfRvO+FaCkBrYHWp7TVBWXXVwt3XAwTPzYPyavd7CC4TnABMoZrXO7iUMhvIAj4AlgHb3b0gOKR0vUrqHOzfATSp1ICP3JPA3UBRsN2E6l3fYg5MNLMZZjY0KIvp33aNIwg2XlkZZYk4Lrda/R7MrC7wGvAzd99pVlb1IoeWURZ39Xb3QqCXmTUE3gC6lXVY8BzXdTazS4Asd59hZmcUF5dxaLWo735Ocfd1ZtYc+MDMvi7n2AqpdyK2FNYAbUtttwHWhRRLZdhoZi0BguesoLza/B7MLIVIQnjZ3V8Piqt9vQHcfTvwMZH+lIZmVvxFr3S9Suoc7G8AbK3UQI/MKcAPzGwFMI7IJaQnqb71LeHu64LnLCLJvy8x/ttOxKQwDegUjFxIBa4B3g45plh6G7gheH0DkWvuxeWDghEL/YEdxU3SeGKRJsEIINPdHy+1q9rW28yaBS0EzKwWcA6RDtjJwBXBYfvXufh3cQXwkQcXneOBu9/r7m3cPZ3I/9eP3P06qml9i5lZHTOrV/waOA+YT6z/tsPuSAmp8+YiYDGR67C/CTueCqzXK8B6IJ/It4YhRK6lTgKWBM+Ng2ONyCisZcA8ICPs+A+zzqcSaSLPBWYHj4uqc72BnsCsoM7zgfuD8g7AVGAp8C8gLSivGWwvDfZ3CLsOR1D3M4B3EqG+Qf3mBI8FxZ9Vsf7b1jQXIiJSIhEvH4mIyAEoKYiISAklBRERKaGkICIiJZQURESkhJKCVHlm5mb2WKntX5rZ7yvovUea2RUHP/KIf86VwUyuk/crTw/q92CpsqZmlm9m/4h1XCL7U1KQeJALXGZmTcMOpLRgxt1oDQFuc/czy9j3DXBJqe0riYxLP5RYEnHKGokBJQWJBwVE1qP9+f479v+mb2a7g+czzOwTMxtvZovN7K9mdp1F1iGYZ2YdS73NOWb2WXDcJcH5yWb2iJlNC+amv6XU+042s7FEbhDaP55rg/efb2Z/C8ruJ3KT3bNm9kgZ9dsLZJpZ8dTIVwPjS71nMzN7LYhlmpmdEpT/3syGmdlEYHRw3AdmNtPMnjOzlcWJ1MyuD+o+O9iXHDxGBrHOM7Pv/H4l8ejbhcSLp4G5ZvbwIZxzPJGJ4rYS+TY+3N37WmQhnp8CPwuOSwdOBzoCk83sGGAQkWkC+phZGvBF8OELkflnerj78tI/zMxaEZm7vzeR+f0nmtml7v5HMzsL+KW7Tz9ArOOAa8xsA1BIZM6aVsG+p4isG/C5mbUjMjV08QR4vYFT3X1vcLnpI3f/i5ldQGROfcysG5FEc4q755vZP4HriLRGWrt7j+C4hlH+XqUaU1KQuOCRmU9HA3cQ+WYdjWkezP1iZsuA4g/1eUDpyzjj3b0IWGJm3wBdicwz07NUK6QBkcVL8oCp+yeEQB/gY3ffFPzMl4HTgDejiPU94EFgI/DqfvvOAbrbtzO/1i+eEwd4292Lfx+nAj8EcPf3zGxbUH42keQxLXiPWkQmUfsP0MHM/hd4l29/P5LAlBQknjxJZEGZF0uVFRBcBg0mx0sttS+31OuiUttF7Pu3v/9cL05kHpmfuvv7pXdYZOrm7APEd8D5ug/G3fPMbAZwF3As8P1Su5OILBqzTzIMPuBLx3Kgn2/AKHe/9zs7zI4HzgduB64Cfny4dZDqQX0KEjfcfSuRa+1DShWvIPItGGAAkHIYb32lmSUF/QwdiKxY9T7wPxaZlhsz6xzMVFmeKcDpweihZOBa4JNDiOMx4NfuvmW/8onAT4o3zKzXAc7/nMgHO2Z2HtAoKJ8EXGGROfmL1/htH/Q3JLn7a8DvgLhbr1oqnloKEm8eo9QHJPA88JaZTSXy4Xegb/HlWUTkw7sFcKu755jZcCJ9DTODFsgm4NLy3sTd15vZvUSmdDZggru/Vd45+52/gLJHHd0BPG1mc4n8n/0UuLWM4/4AvGJmVwf1WQ/scvfNZvZbIn0cSURm0b2dyGW4F4MyiCxNKwlOs6SKVBNBh3ihuxeY2UnAM+7eK+SwJM6opSBSfbQDxgff/POAm0OOR+KQWgoiIlJCHc0iIlJCSUFEREooKYiISAklBRERKaGkICIiJf4fE1291+cjThAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Number of Merges\")\n",
    "plt.ylabel(\"Number of Tokens\")\n",
    "plt.plot(range(len(vocab_lengths)), vocab_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Decoding\n",
    "Once we have determined our subword pairings\n",
    "- how do we encode sentences?\n",
    "- how do we decode encoded sentences?\n",
    "- what do we tokenize first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Sentences\n",
    "Encoding Sentenes is fairly tricky. BPE is a greedy algorithm, meaning the first tokens we encode are the most frequent tokens. The code works out roughly to \n",
    "- split the sentence into a list of words\n",
    "    - replace subword tokens within word until \n",
    "        - either we have traverse the full vocabulary\n",
    "        - or the word is full tokenized\n",
    "    - if there are any characters at the end which are not recognized as tokens, replace them with \"\\<u\\>\" or \"unknown token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_encode(word, sorted_tokens):\n",
    "    \"\"\" wrap all subword tokens with || and unkown tokens with <u>\"\"\"   \n",
    "    word = re.escape(word)\n",
    "    for token in sorted_tokens:\n",
    "        \n",
    "        if token in word:\n",
    "            subwords = re.split(\"(\\|.*?\\|)\", word)\n",
    "            # exit case:\n",
    "            # subword tokens already found\n",
    "            if all([subword.startswith(\"|\") for subword in subwords if subword]):\n",
    "                break\n",
    "                \n",
    "            new_word = []\n",
    "            for subword in subwords:\n",
    "                # edge case: treat </w> as a single character\n",
    "                if token == \"w\" and \"</w>\" in subword and not subword.startswith(\"|\"):\n",
    "                    eow_idx = subword.find(\"</w>\")\n",
    "                    subword = subword[:eow_idx].replace(token, \"|{}|\".format(token)) + \"</w>\"\n",
    "                \n",
    "                # if subword is already a token, skip\n",
    "                elif not subword.startswith(\"|\"):\n",
    "                    subword = subword.replace(token, \"|{}|\".format(token))\n",
    "\n",
    "                new_word.append(subword)\n",
    "\n",
    "            word = ''.join(new_word)\n",
    "            \n",
    "    # replace all tokens not in vocab with unknown\n",
    "    subwords = re.split(\"(\\|.*?\\|)\", word)\n",
    "    new_word = []\n",
    "    for subword in subwords:\n",
    "        if subword:\n",
    "            if not subword.startswith(\"|\"):\n",
    "                subword = subword.replace(subword, \"|<u>|\")\n",
    "            new_word.append(subword)\n",
    "    word = ''.join(new_word)\n",
    "    \n",
    "    return word\n",
    "\n",
    "def encode(sentence, sorted_tokens):\n",
    "    \"\"\" \n",
    "    Split a sentence into word and subword tokens.\n",
    "    If the entire word is in our vocabulary, the thats the token\n",
    "    else break the word into subword tokens\n",
    "    \"\"\"\n",
    "    # preprocess sentence in same way vocab is created\n",
    "    sentence = sentence.strip().replace(\"\\n\", \"\").lower()\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    words = sentence.split()\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        word = word + \"</w>\"\n",
    "        subwords = subword_encode(word, sorted_tokens)\n",
    "        tokens.append(subwords)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Sentences\n",
    "Unlike encoding, decoding lists of tokens into sentences is fairly easy! Just remove the \\\"\\|\\\" and \\\"\\<w\\>\\\" characters and concatenate the sentence together (with spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens):\n",
    "    \"\"\" reconstruct sentence from original word \"\"\"\n",
    "    decoded_result = []\n",
    "    for token in tokens:\n",
    "        token = token.replace(\"|\", \"\").replace(\"</w>\", \"\")\n",
    "        decoded_result.append(token)\n",
    "    return \" \".join(decoded_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Demonstration\n",
    "Now that we have implemented our encoding and decoding, lets see if BPE is working! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "              Input: current of his thoughts that anatole is costing me forty thousand              \n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Encoding:\n",
      "\tcurrent:|c||u||r||re||n||t||</w>|\n",
      "\tof:|of</w>|\n",
      "\this:|h||i||s||</w>|\n",
      "\tthoughts:|t||h||o||u||g||h||t||s||</w>|\n",
      "\tthat:|that</w>|\n",
      "\tanatole:|a||n||a||t||o||l||e||</w>|\n",
      "\tis:|i||s||</w>|\n",
      "\tcosting:|c||o||s||t||i||n||g||</w>|\n",
      "\tme:|m||e||</w>|\n",
      "\tforty:|f||o||r||t||y</w>|\n",
      "\tthousand:|t||h||o||u||s||and</w>|\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "         Decoded Output: current of his thoughts that anatole is costing me forty thousand          \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# step 1) get the most common tokens in the vocab\n",
    "tokens_frequency, tokenized_vocab = get_tokens(final_vocab)\n",
    "sorted_tokens_tuple = sorted(tokens_frequency.items(), key=lambda x : x[1], reverse=True)\n",
    "sorted_tokens, counts = zip(*sorted_tokens_tuple)\n",
    "\n",
    "# step 2) select random input sentence\n",
    "src = random.choice(example_document)\n",
    "\n",
    "# step 3) encode sentence using sorted tokens\n",
    "# note: order matters (see weaknesses section below)\n",
    "encoding = encode(src, sorted_tokens)\n",
    "\n",
    "# step 4) decode encoded sentence\n",
    "# should be same as input sentence\n",
    "decoding = decode(encoding)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Input: {}\".format(src).center(100))\n",
    "print(\"=\"*100)\n",
    "print(\"=\"*100)\n",
    "print(\"Encoding:\")\n",
    "for original, encoded in zip(src.split(), encoding):\n",
    "    print(\"\\t{}:{}\".format(original, encoded))\n",
    "print(\"=\"*100)\n",
    "print(\"=\"*100)\n",
    "print(\"Decoded Output: {}\".format(src).center(100))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses of BPE\n",
    "With BPE one of the largest downsides is ambiguity.\n",
    "- Ambiguity - sometimes there are several different ways to use a byte pair table to encode a single word or phrase. For example imagine out list of subword tokens is \n",
    "$$\\text{[c, a, t, at]}$$\n",
    "thus in this context the word cat can be broken down as either \n",
    "$$\\text{[c, a, t] or [c, at]}$$\n",
    "\n",
    "Note that BPE is deterministic. It might be ambiguous to us which tokenization method BPE will chose, but it will always choose the same one, the goal then is not to prohibit this from happening but to make sure the tokens which are chosen are the correct tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Lei Mao: Byte Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding)\n",
    "- [The Original Paper](https://arxiv.org/pdf/1508.07909.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
